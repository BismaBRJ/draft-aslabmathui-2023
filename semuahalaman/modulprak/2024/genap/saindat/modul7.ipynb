{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modul 7 Praktikum Sains Data: Pengantar TensorFlow & Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kembali ke [Sains Data](./saindat2024genap.qmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita sudah masuk ke materi *artificial neural network* (ANN) atau biasa disebut *neural network* (NN), yang mendasari dunia *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saat modul praktikum ini disusun (April 2024), ada dua *framework* utama untuk *deep learning* di Python, yaitu:\n",
    "\n",
    "1. TensorFlow: <https://www.tensorflow.org/>\n",
    "\n",
    "    (dan Keras di dalamnya: <https://keras.io/>)\n",
    "\n",
    "2. PyTorch: <https://pytorch.org/>\n",
    "\n",
    "Kedua *framework* ini bersaing. Umumnya, TensorFlow lebih sering digunakan di industri, sedangkan PyTorch lebih sering digunakan dalam riset/penelitian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di pertemuan kali ini, kita akan membahas TensorFlow, baik penggunaannya secara sendiri (*pure TensorFlow*, yaitu tanpa Keras) maupun dengan bantuan Keras. Kalau belum punya, instal terlebih dahulu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras terinstal bersama TensorFlow (karena Keras ada di dalamnya)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengantar *Neural Network*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Overview:*\n",
    "\n",
    "- Secara umum, suatu *neural network* terdiri dari sejumlah *layer* atau lapisan (minimal dua).\n",
    "\n",
    "- *Layer* pertama disebut *input layer*, dan *layer* terakhir disebut *output layer*.\n",
    "\n",
    "- Tiap *layer* terdiri dari sejumlah *neuron*, yang masing-masing bisa menyimpan suatu nilai.\n",
    "\n",
    "- Kecuali *input layer*, tiap *neuron* terhubung dengan sejumlah *neuron* di *layer* sebelumnya.\n",
    "\n",
    "- Tiap sambungan terdiri dari nilai *weight* (sebagai pengali), nilai *bias* (sebagai pergeseran), dan suatu \"fungsi aktivasi\" yang menghasilkan nilai untuk *neuron* tujuan.\n",
    "\n",
    "- *Weight* maupun *bias* disebut **parameter** dari *neural network*.\n",
    "\n",
    "- Proses *training* adalah terus-menerus memperbarui parameter hingga hasil prediksi *neural network* sudah cukup baik, dengan meminimumkan suatu ***loss function*** atau fungsi objektif (yang intinya menghitung *error*).\n",
    "\n",
    "- Suatu *neural network* bisa memiliki sejumlah *layer*, masing-masing dengan banyaknya *neuron* tertentu dan fungsi-fungsi aktivasi tertentu. Hal-hal itu disebut \n",
    "***hyperparameter*** dari *neural network*. Suatu **arsitektur** adalah suatu pilihan/konfigurasi *hyperparameter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLP: *(Single-Layer) Perceptron*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN paling pertama adalah *perceptron* (juga disebut SLP atau *single-layer perceptron*) yang dirancang oleh Frank Rosenblatt pada tahun 1957 (GÃ©ron, 2019). Ini adalah *neural network* yang paling sederhana, bahkan ini bisa disebut *building block* dari semua ANN (apabila diberi kebebasan untuk modifikasi). Konsep dasar *neural network* bisa kita pelajari di sini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gambar/aggarwal_nndl_hal5.png)\n",
    "\n",
    "Sumber gambar: Aggarwal (2018) hal. 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Perceptron* hanya terdiri dari satu *input layer* dan satu *output layer*. Bahkan, aslinya hanya ada satu *neuron* di *output layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila dibutuhkan lebih dari satu *neuron* di *output layer*, itu bisa dianggap menggunakan lebih dari satu *perceptron* (yaitu menggunakan banyaknya *perceptron* sesuai banyaknya *neuron* di *output layer*), yang saling \"ditumpuk\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gambar/goodfellow_dl_hal337_crop.png)\n",
    "\n",
    "Sumber gambar: Goodfellow, et. al. (2016) hal. 337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aslinya, fungsi aktivasi yang digunakan oleh *perceptron* adalah *Heaviside step function* $H(v)$ yang mungkin kalian kenal dari mata kuliah PDB, atau juga disebut *threshold activation function*:\n",
    "\n",
    "$$H(v) = \\begin{cases}\n",
    "    1, & v \\ge 0 \\\\\n",
    "    0, & v < 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehingga, untuk *output neuron* ke-$j$ yang disambung dari $n$ *input neuron*, model *perceptron* bisa dirumuskan sebagai berikut:\n",
    "\n",
    "$$y_j = H\\left(\\left(\\sum_{i=1}^{n} w_{ij} x_i \\right) + b_j\\right)$$\n",
    "\n",
    "dengan\n",
    "\n",
    "- $x_i$ adalah nilai pada *input neuron* ke-$i$\n",
    "\n",
    "- $y_j$ adalah nilai pada *output neuron* ke-$j$\n",
    "\n",
    "- $w_{ij}$ adalah parameter *weight* untuk sambungan *input neuron* ke-$i$ menuju *output neuron* ke-$j$\n",
    "\n",
    "- $b_j$ adalah parameter *bias* untuk *output neuron* ke-$j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lebih umumnya,\n",
    "\n",
    "$$y_j = \\Phi\\left(\\left(\\sum_{i=1}^{n} w_{ij} x_i \\right) + b_j\\right)$$\n",
    "\n",
    "dengan $\\Phi(v)$ adalah sembarang fungsi aktivasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* seperti di gambar, sebenarnya *bias* juga bisa dianggap *neuron* istimewa yang nilai $x_i$ nya selalu satu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biasanya, semua nilai di *layer* selanjutnya dihitung secara sekaligus menggunakan perkalian matriks, dengan perumusan:\n",
    "\n",
    "$$\\textbf{y} = \\Phi\\left(\\textbf{w}^T \\textbf{x} + \\textbf{b}\\right)$$\n",
    "\n",
    "dengan $\\textbf{x} = [x_i]$, $\\textbf{y} = [y_j]$, dan $\\textbf{b} = [b_j]$ adalah vektor kolom, serta $\\textbf{w} = \\left[w_{ij}\\right]$ adalah matriks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP: *Multilayer Perceptron*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konsep *single-layer perceptron* bisa diperumum menjadi *multilayer perceptron* atau *neural network* yang biasa kita kenal, dengan menambahkan beberapa *layer* di antara *input layer* dan *output layer*. Semua *layer* selain *input layer* dan *output layer* disebut *hidden layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gambar/aggarwal_nndl_hal18.png)\n",
    "\n",
    "Sumber gambar: Aggarwal (2018) hal. 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konsep perhitungan antara tiap *layer* tetap sama, yaitu\n",
    "\n",
    "$$\\textbf{y} = \\Phi\\left(\\textbf{w}^T \\textbf{x} + \\textbf{b}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi Aktivasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gambar/aggarwal_nndl_hal13.png)\n",
    "\n",
    "Sumber gambar: Aggarwal (2018) hal. 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beberapa fungsi aktivasi adalah (Aggarwal, 2018, hal. 12-13):\n",
    "\n",
    "a. \"Linier\" atau identitas\n",
    "\n",
    "$$\\Phi(v) = v$$\n",
    "\n",
    "b. *Sign* (fungsi tanda): $\\text{sign}(v)$ atau $\\text{sgn}(v)$\n",
    "\n",
    "$$\n",
    "\\Phi(v) = \\text{sign}(v) = \\begin{cases}\n",
    "    1, & v > 0 \\\\\n",
    "    0, & v = 0 \\\\\n",
    "    -1, & v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "c. Sigmoid, terkadang dilambangkan $\\sigma(v)$ dan terkadang disebut fungsi aktivasi logistik\n",
    "\n",
    "$$\\Phi(v) = \\frac{1}{1 + e^{-v}}$$\n",
    "\n",
    "d. *(Soft)* tanh: $\\tanh(v)$\n",
    "\n",
    "$$\\Phi(v) = \\frac{e^{2v} - 1}{e^{2v} + 1} = 2 * \\text{sigmoid}(2v) - 1$$\n",
    "\n",
    "e. *Rectified Linear Unit* (ReLU)\n",
    "\n",
    "$$\\Phi(v) = \\max\\{v, 0\\}$$\n",
    "\n",
    "f. *Hard tanh*\n",
    "\n",
    "$$\\Phi(v) = \\max\\{\\min\\{v, 1\\}, -1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi aktivasi yang paling sering digunakan adalah ReLU, kecuali untuk *output layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk *output layer*, biasanya,\n",
    "\n",
    "- untuk regresi, banyaknya *neuron* sesuai banyaknya nilai prediksi (umumnya hanya satu), dan digunakan fungsi aktivasi linier\n",
    "\n",
    "- untuk klasifikasi *multiclass* (lebih dari dua kelas), biasanya banyaknya *output neuron* sesuai banyaknya kelas, dan digunakan fungsi aktivasi ***softmax*** sebagai berikut, agar *output* berupa peluang tiap kelas:\n",
    "\n",
    "$$\\Phi(\\overline{v})_i = \\frac{\\exp(v_i)}{\\sum_{j=1}^k \\exp(v_j)}$$\n",
    "\n",
    "- untuk klasifikasi biner, hanya ada satu *neuron* di *output layer*, dan digunakan fungsi aktivasi *sigmoid*. (Keberadaan hanya satu *output neuron* lebih hemat daripada menggunakan dua *output neuron*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Loss function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misalkan $y_i$ adalah nilai sebenarnya dan $\\hat{y}_i$ adalah hasil prediksi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk regresi, biasa digunakan MSE *(mean squared error)*, juga disebut *L2 loss*:\n",
    "\n",
    "$$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk klasifikasi, biasa digunakan yang namanya *cross-entropy loss*, juga disebut *logistic loss* atau *log loss*:\n",
    "\n",
    "$$L_{\\text{log}}(y,\\hat{y}) = -(y \\ln (\\hat{y}) + (1 - y) \\ln (1 - \\hat{y}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proses *training*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses *training* untuk *neural network* dilakukan secara iteratif, yaitu tiap iterasi akan memperbarui parameter sehingga nilai *loss function* menjadi lebih kecil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiap iterasi melakukan langkah-langkah berikut untuk tiap data *training*:\n",
    "\n",
    "1. *Forward pass*: menghitung nilai *output* akhir, yaitu $\\hat{y}$ (hasil prediksi), berdasarkan *input* data *training*.\n",
    "\n",
    "2. Menghitung *loss* antara $y$ (nilai asli) dan $\\hat{y}$\n",
    "\n",
    "3. *Backpropagation*: menghitung gradien dari *loss* terhadap tiap parameter, secara \"mundur\"\n",
    "\n",
    "4. *Update optimizer*: menggunakan algoritma *optimizer* seperti *gradient descent* untuk memperbarui parameter-parameter (*weights and biases*) berdasarkan gradien dari *loss*\n",
    "\n",
    "    Note: ada banyak *optimizer*, seperti *gradient descent*, SGD (*stochastic gradient descent*), dan *Adam* (*adaptive moment estimation*). Pilihan *optimizer* (serta parameter-parameter yang bisa diatur untuk *optimizer*, seperti *learning rate*) juga menjadi *hyperparameter* untuk *neural network*.\n",
    "\n",
    "Note: istilah *backward pass* meliputi langkah *backpropagation* dan *update optimizer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila data *training* sangat banyak, terkadang data *training* tersebut dibagi menjadi beberapa *batch*, dan tiap iterasi menggunakan *batch* yang berbeda. Apabila semua *batch* sudah diproses, sebutannya adalah satu ***epoch***. Sehingga, satu *epoch* terdiri dari sejumlah iterasi sesuai banyaknya *batch*.\n",
    "\n",
    "(Apabila data *training* tidak dibagi menjadi *batch*, maka satu *epoch* sama dengan satu iterasi.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *(Pure)* TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variabel dan Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Automatic differentiation* dengan `GradientTape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasifikasi biner dengan *perceptron*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Sequential API*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Functional API*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Subclassing API* (yaitu dengan OOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referensi\n",
    "\n",
    "Sumber gambar\n",
    "\n",
    "- Aggarwal, C. Charu. 2018. *Neural Networks and Deep Learning: A Textbook.* Edisi Pertama. Springer.\n",
    "\n",
    "- Goodfellow, Ian; Bengio, Yoshua; & Courville, Aaron. 2016. *Deep Learning*. MIT Press.\n",
    "\n",
    "Buku lainnya\n",
    "\n",
    "- GÃ©ron, AurÃ©lien. 2019. *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems.* Edisi Kedua. O'Reilly Media.\n",
    "\n",
    "Internet\n",
    "\n",
    "- <https://www.tensorflow.org/api_docs/python/tf/GradientTape>\n",
    "\n",
    "- <https://keras.io/api/models/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
