{
  "hash": "c40b34b469b971af98e3bafbac9d8f1c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pertemuan 8: Uji Cramer von Mises, Friedman, Quade, Kruskal Wallis, Correlation, Regresi Nonparametrik, Regresi Monotonik, Kernel Smoothing, Regresi Spline\"\nsubtitle: \"Statistika Nonparametrik\"\nimage: static\\regresi_monoton.png\ndescription: \"Offline di Departemen Matematika\"\ndate: 12/03/2024\npage-navigation: true\nformat: html\n---\n\n\n# Tabel Guide Uji Nonparametrik\n\n![](static\\tabelconover1.png)\n\n![](static\\tabelconover2.png)\n\n# Uji Cramer von Mises\n\nLindsey, Herzberg dan Watts (1987) memberikan data lebar ruas pertama tarsus kedua untuk dua spesies serangga Chaetocnema. Apakah ini menunjukkan perbedaan populasi antara distribusi lebar untuk kedua spesies?\n\n|Species A|131|134|137|127|128|118|134|129|131|115|\n|---|---|---|---|---|---|---|---|---|---|---|\n|Species B|107|122|144|131|108|118|122|127|125|124|\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame (Species_A  = c(131,134,137,127,128,118,134,129,131,115),\n                  Species_B = c(107,122,144,131,108,118,122,127,125,124)\n)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Species_A Species_B\n1        131       107\n2        134       122\n3        137       144\n4        127       131\n5        128       108\n6        118       118\n7        134       122\n8        129       127\n9        131       125\n10       115       124\n```\n\n\n:::\n:::\n\n\nTujuan: Akan dilakukan pengujian Cramer Von Mises untuk mengetahui apakah ada perbedaan distribusi antar populasi\n\nHipotesis:\n\n$H_0: F(x) = G(x)$ for all $x$ from $- \\infty$ to  $+ \\infty$\n\n$H_1: F(x) \\neq G(x)$ for at least one value of $x$\n\ndapat digunakan fungsi `cvm_test()` dari library `twosamples`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages('twosamples')\nlibrary(twosamples)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncvm_test(df$Species_A,df$Species_B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest Stat   P-Value \n   1.5500    0.0845 \n```\n\n\n:::\n:::\n\n\nKesimpulannya?\n\n# Uji Friedman\n\nUntuk kelompok tujuh siswa, denyut nadi (per menit) diukur sebelum latihan (I), segera setelah latihan (II), dan 5 menit setelah latihan (III). Data diberikan pada di bawah. Gunakan uji Friedman untuk menguji perbedaan antara denyut nadi pada tiga kesempatan. \n\n![](static\\Friedman_table.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataset_Friedmann <- data.frame(Student = c('A','A','A','B','B','B','C','C','C','D','D','D','E','E','E','F','F','F','G','G','G'),\n                                Time = rep(c(1,2,3), 7),\n                                Score = c(72, 120, 76, 96, 120, 95, 88, 132, 104, 92, 120, 96, 74, 101, 84, 76, 96, 72, 82,  112, 76))\n```\n:::\n\n\nTujuan: Akan dilakukan pengujian Friedman untuk mengetahui apakah terdapat perbedaan denyut nadi per menit pada sebelum latihan, setelah latihan, dan 5 menit setelah latihan untuk kelompok 7 siswa tersebut\n\nHipotesis:\n\n$H_0:$ All treatments have identical effects\n\n$H_1:$ At least one treatment yield larger observed values than at least one other treatment\n\ndapat digunakan fungsi `friedman.test()` dari library `stats`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfriedman.test(y=Dataset_Friedmann$Score, groups=Dataset_Friedmann$Time, blocks=Dataset_Friedmann$Student)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFriedman rank sum test\n\ndata:  Dataset_Friedmann$Score, Dataset_Friedmann$Time and Dataset_Friedmann$Student\nFriedman chi-squared = 10.571, df = 2, p-value = 0.005063\n```\n\n\n:::\n:::\n\n\nKesimpulannya?\n\n# Uji Quade\n\nUji Quade adalah alternatif bagi Uji Friedman. Kedua pengujian memiliki hipotesis null yang sama.\n\nMisal ingin diuji data dari soal pada section Uji Friedmann menggunakan Uji Quade\n\ndapat digunakan fungsi `quade.test()` dari library `stats`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquade.test(Score ~ Time | Student,\n           data = Dataset_Friedmann)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tQuade test\n\ndata:  Score and Time and Student\nQuade F = 10.517, num df = 2, denom df = 12, p-value = 0.002298\n```\n\n\n:::\n:::\n\n\nDengan menggunakan fungsi `quadeAllPairsTest()` dari library `PMCMRplus`, kita bisa melakukan *Quade multiple-comparison test*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(PMCMRplus)\nlibrary(PMCMRplus)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nquadeAllPairsTest(y      = Dataset_Friedmann$Score,\n                       groups = Dataset_Friedmann$Time,\n                       blocks = Dataset_Friedmann$Student,\n                       p.adjust.method = \"fdr\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n\tPairwise comparisons using Quade's test with TDist approximation\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ndata: Dataset_Friedmann$Score, Dataset_Friedmann$Time and Dataset_Friedmann$Student\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1      2     \n2 0.0026 -     \n3 0.2922 0.0094\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nP value adjustment method: fdr\n```\n\n\n:::\n:::\n\n\nHasil menunjukkan pasangan grup yang berbeda (atau tidak berbeda) secara signifikan\n\n# Uji Kruskal Wallis\n\nLubischew (1962) memberikan pengukuran lebar kepala maksimum dalam satuan 0,01 mm untuk tiga spesies Chaetocnema. Sebagian dari datanya diberikan di bawah ini. Gunakan uji Kruskalâ€“Wallis untuk melihat apakah ada perbedaan lebar kepala untuk ketiga spesies Chaetocnema. \n\n![](static\\Kruskal_wallis_table.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame (Species  = rep(c(\"Species_1\",\"Species_2\",\"Species_3\"),times=c(10,11,8)),\n                  lebar = c(53,50,52,50,49,47,54,51,52,57,49,49,47,54,43,51,49,51,50,46,49,58,51,45,53,49,51,50,51))\n```\n:::\n\n\nHipotesis:\n\n$H_0:$ All k population distribution functions are identical\n\n$H_1:$ At least one population yield larger observed values than at least one other population\n\nAtau alternatif $H_1$ lainnya\n\n$H_1:$ The k populations do not all have identical means\n\ndapat digunakan fungsi `kruskal.test()` dari library `stats`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(lebar ~ Species, data = df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  lebar by Species\nKruskal-Wallis chi-squared = 4.436, df = 2, p-value = 0.1088\n```\n\n\n:::\n:::\n\n\nKesimpulannya\n\n# Correlation\n\nBardsley dan Chambers (1984) memiliki jumlah sapi ternak (potong) dan domba pada 19 peternakan besar di suatu wilayah. Apakah ada bukti korelasi antara sapi dan domba? \n\n![](static/Correlation_table.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame (Cattle  = c(41,0,42,15,47,0,0,0,56,67,707,368,231,104,132,200,172,146,0),\n                  sheep = c(4716,4605,4951,2745,6592,8934,9165,5917,2618,1105,150,2005,3222,7150,8658,6304,1800,5270,1537))\n```\n:::\n\n\n## Spearman's $\\rho$\n\nUji korelasi *Spearman's $\\rho$* dapat dilakukan dengan fungsi `cor.test()` dari library `stats` dengan method `\"spearman\"`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$Cattle, df$sheep, method=\"spearman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cor.test.default(df$Cattle, df$sheep, method = \"spearman\"): Cannot\ncompute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tSpearman's rank correlation rho\n\ndata:  df$Cattle and df$sheep\nS = 1517.3, p-value = 0.1663\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3309864 \n```\n\n\n:::\n:::\n\n\n## Kendall's $\\tau$\n\nUji korelasi *Kendall's $\\tau$* dapat dilakukan dengan fungsi `cor.test()` dari library `stats` dengan method `\"kendall\"`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(df$Cattle, df$sheep, method=\"kendall\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cor.test.default(df$Cattle, df$sheep, method = \"kendall\"): Cannot\ncompute exact p-value with ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKendall's rank correlation tau\n\ndata:  df$Cattle and df$sheep\nz = -1.3786, p-value = 0.168\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.2350464 \n```\n\n\n:::\n:::\n\n\n# Regresi Nonparametrik\n\nA driver kept track of the number of miles she traveled and the number of gallons put in the tank each time she bought gasoline. \n\n| Miles | Gallons |\n|-------|---------|\n| 142   | 11.1    |\n| 116   | 5.7     |\n| 194   | 14.2    |\n| 250   | 15.8    |\n| 88    | 7.5     |\n| 157   | 12.5    |\n| 255   | 17.9    |\n| 159   | 8.8     |\n| 43    | 3.4     |\n| 208   | 15.2    |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMiles = c(142, 116, 194, 250, 88, 157, 255, 159, 43, 208)\nGallons = c(11.1, 5.7, 14.2, 15.8, 7.5, 12.5, 17.9, 8.8, 3.4, 15.2)\ndf = data.frame(Gallons, Miles)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Gallons Miles\n1     11.1   142\n2      5.7   116\n3     14.2   194\n4     15.8   250\n5      7.5    88\n6     12.5   157\n7     17.9   255\n8      8.8   159\n9      3.4    43\n10    15.2   208\n```\n\n\n:::\n:::\n\n\nLibrary yang akan digunakan:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stats)\nlibrary(ggplot2)\nlibrary(ggpmisc)\n```\n:::\n\n\nA. Draw a diagram showing these points, using gallons as the x-axis \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Gallons, Miles, xlab = \"Gallons\", ylab = \"Miles\", \n     main = \"Gallons x Miles\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nB. Estimate a and b using the method of least squares \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 = lm(Miles ~ Gallons, data=df)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Miles ~ Gallons, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-22.73 -16.26  -7.68  20.46  30.59 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    8.692     19.155   0.454    0.662    \nGallons       13.605      1.585   8.582 2.63e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.6 on 8 degrees of freedom\nMultiple R-squared:  0.902,\tAdjusted R-squared:  0.8898 \nF-statistic: 73.64 on 1 and 8 DF,  p-value: 2.626e-05\n```\n\n\n:::\n:::\n\n\nC. Plot the least squares regression line on the diagram of part A \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Gallons, Miles, xlab = \"Gallons\", ylab = \"Miles\", \n     main = \"Gallons x Miles\", lwd = 2)\nabline(reg = model1, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nD. Suppose the EPA estimated this car's mileage at 18 miles per gallon. Test the null hypothesis that this figure applies to this particular car and driver. (Use the test for slope) \n\n$H_0:$ Jarak tempuh mobil ($\\beta$) adalah 18 mil/galon\n\n$H_1:$ Jarak tempuh mobil ($\\beta$) bukan 18 mil/galon\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults = list()\nbeta0 = 18\n\nfor (i in 1:length(df$Miles)) {\n  results[[i]] <- df$Miles[i] - beta0 * df$Gallons[i]\n}\nresults = unlist(results)\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -57.8  13.4 -61.6 -34.4 -47.0 -68.0 -67.2   0.6 -18.2 -65.6\n```\n\n\n:::\n\n```{.r .cell-code}\nstat_uji = cor(Gallons, results, method = \"spearman\")\nstat_uji\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.6606061\n```\n\n\n:::\n\n```{.r .cell-code}\nabs(stat_uji) > 0.6364\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nE. Find a 95% confidence interval for the mileage of this car and driver\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate pairwise slopes\ncalculate_slopes <- function(X, Y) {\n  slopes <- c()\n  n <- length(X)\n  \n  for (i in 1:(n - 1)) {\n    for (j in (i + 1):n) {\n      if (X[j] != X[i]) {  # Avoid division by zero\n        S_ij <- (Y[j] - Y[i]) / (X[j] - X[i])\n        slopes <- c(slopes, S_ij)\n      }\n    }\n  }\n  return(slopes)\n}\n\n# Calculate pairwise slopes\nslopes <- calculate_slopes(df$Gallons, df$Miles)\n\n# Median slope (Theil-Sen estimator)\nmedian_slope <- median(slopes)\n\n# 95% confidence interval\nlower_bound <- quantile(slopes, 0.025)  # 2.5th percentile\nupper_bound <- quantile(slopes, 0.975)  # 97.5th percentile\n\n# Output results\nlist(\n  median_slope = median_slope,\n  lower_bound = lower_bound,\n  upper_bound = upper_bound\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$median_slope\n[1] 14\n\n$lower_bound\n     2.5% \n-6.706228 \n\n$upper_bound\n   97.5% \n52.65385 \n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Model Alternatif\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Median slope (Theil-Sen estimator)\nmedian_slope <- median(slopes)\n```\n:::\n\n\nPada metode kalkulasi confidence interval di atas dapat digunakan untuk membentuk suatu estimasi persamaan regresi nonparametrik dengan median Y dan X sebagai estimator intercept dan $\\beta$ secara berturut-turut.\n:::\n\n# Regresi Monotonik\n\nMetode regresi linier nonparametrik dapat digunakan ketika asumsi regresi linier dapat dipenuhi. Dalam situasi dimana **tidak dapat diasumsikan bahwa garis regresi adalah linier tapi dapat diasumsikan bahwa E(Y|X) naik (minimal tidak turun) dengan meningkatnya X**. Dalam hal ini dinamakan regresinya naik secara monoton.\n\n| Xi  | Yi |\n|-----|----|\n| 0.0 | 30 |\n| 0.5 | 30 |\n| 1.0 | 30 |\n| 1.8 | 28 |\n| 2.2 | 24 |\n| 2.7 | 19 |\n| 4.0 | 17 |\n| 4.0 | 9  |\n| 4.9 | 12 |\n| 5.6 | 12 |\n| 6.0 | 6  |\n| 6.5 | 8  |\n| 7.3 | 4  |\n| 8.0 | 5  |\n| 8.8 | 6  |\n| 9.3 | 4  |\n| 9.8 | 6  |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data extracted from the table\ndata <- data.frame(\n  Xi = c(0, 0.5, 1.0, 1.8, 2.2, 2.7, 4.0, 4.0, 4.9, 5.6, 6.0, 6.5, 7.3, 8.0, 8.8, 9.3, 9.8),\n  Yi = c(30, 30, 30, 28, 24, 19, 17, 9, 12, 12, 6, 8, 4, 5, 6, 4, 6)\n)\n\ndata$R_Xi <- rank(data$Xi, ties.method = \"average\")\ndata$R_Yi <- rank(data$Yi, ties.method = \"average\")\n\nn <- nrow(data)\nmean_R_Xi <- mean(data$R_Xi)\nmean_R_Yi <- mean(data$R_Yi)\n\n# Calculate b2 (slope)\nb2 <- sum((data$R_Xi - mean_R_Xi) * (data$R_Yi - mean_R_Yi)) /\n      sum((data$R_Xi - mean_R_Xi)^2)\n\n# Calculate a2 (intercept)\na2 <- mean_R_Yi - b2 * mean_R_Xi\n\n# Estimate R(Y) for Given R(X)\ndata$Rhat_Yi <- a2 + b2 * data$R_Xi\n\n# Convert Rhat_Y back to Y\ninterpolate_Y <- function(R_hat_Y, Yi, R_Yi) {\n  if (R_hat_Y %in% R_Yi) {\n    return(Yi[which(R_Yi == R_hat_Y)])  # Return the Yi if the rank is equal\n  } else if (R_hat_Y < min(R_Yi)) {\n    return(min(Yi))  # Return the largest Yi if the rank is less than the minimum\n  } else if (R_hat_Y > max(R_Yi)) {\n    return(max(Yi))  # Return the largest Yi if the rank is greater than the maximum\n  } else {\n    # Find the nearest ranks for interpolation\n    lower <- max(R_Yi[which(R_Yi < R_hat_Y)])\n    upper <- min(R_Yi[which(R_Yi > R_hat_Y)])\n    \n    Y_lower <- Yi[which(R_Yi == lower)]\n    Y_lower <- Y_lower[1]\n    Y_upper <- Yi[which(R_Yi == upper)]\n    Y_upper <- Y_upper[1]\n    R_lower <- lower\n    R_upper <- upper\n    \n    # Linear interpolation\n    return(Y_lower + (R_hat_Y - R_lower) / (R_upper - R_lower) * (Y_upper - Y_lower))\n  }\n}\n\ndata$Yhat_i <- sapply(data$Rhat_Yi, interpolate_Y, Yi = data$Yi, R_Yi = data$R_Yi)\n\ndata$Rhat_Xi <- (data$R_Yi - a2)/b2\n\ninterpolate_X <- function(R_hat_X, X, R_X) {\n  if (R_hat_X <= min(R_X)) {\n    return(NULL)\n  }\n  if (R_hat_X >= max(R_X)) {\n    return(NULL)\n  }\n  \n  # Find the nearest ranks for interpolation\n  lower <- max(which(R_X <= R_hat_X))\n  upper <- min(which(R_X > R_hat_X))\n  \n  X_lower <- X[lower]\n  X_upper <- X[upper]\n  R_lower <- R_X[lower]\n  R_upper <- R_X[upper]\n  \n  # Linear interpolation\n  return(X_lower + (R_hat_X - R_lower) / (R_upper - R_lower) * (X_upper - X_lower))\n}\n\ndata$Xhat_i <- sapply(data$Rhat_Xi, function(R_hat) interpolate_X(R_hat, data$Xi, data$R_Xi))\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Xi Yi R_Xi R_Yi   Rhat_Yi    Yhat_i   Rhat_Xi    Xhat_i\n1  0.0 30  1.0 16.0 16.469939 30.000000  1.503285 0.2516426\n2  0.5 30  2.0 16.0 15.536196 29.536196  1.503285 0.2516426\n3  1.0 30  3.0 16.0 14.602454 28.602454  1.503285 0.2516426\n4  1.8 28  4.0 14.0 13.668712 26.674847  3.645204  1.516163\n5  2.2 24  5.0 13.0 12.734969 22.674847  4.716163  2.086465\n6  2.7 19  6.0 12.0 11.801227 18.602454  5.787122  2.593561\n7  4.0 17  7.5 11.0 10.400613 15.002045  6.858081  3.443671\n8  4.0  9  7.5  8.0 10.400613 15.002045 10.070959  5.628384\n9  4.9 12  9.0  9.5  9.000000 11.000000  8.464520  4.578712\n10 5.6 12 10.0  9.5  8.066258  9.132515  8.464520  4.578712\n11 6.0  6 11.0  5.0  7.132515  8.132515 13.283837  7.498686\n12 6.5  8 12.0  7.0  6.198773  7.198773 11.141919  6.070959\n13 7.3  4 13.0  1.5  5.265031  6.265031 17.032194      NULL\n14 8.0  5 14.0  3.0  4.331288  5.665644 15.425756  9.012878\n15 8.8  6 15.0  5.0  3.397546  5.198773 13.283837  7.498686\n16 9.3  4 16.0  1.5  2.463804  4.642536 17.032194      NULL\n17 9.8  6 17.0  5.0  1.530061  4.020041 13.283837  7.498686\n```\n\n\n:::\n:::\n\n\nNow we can visualize the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_clean <- data[!sapply(data$Xhat_i, is.null), ]\n\ncombined <- rbind(\n  as.matrix(data[, c('Xhat_i', 'Yi')]), \n  as.matrix(data[, c('Xi', 'Yhat_i')])\n)\n\ncombined <- as.data.frame(combined)\ncolnames(combined) <- c('X', 'Y')\ncombined <- combined[!sapply(combined$X, is.null), ]\ncombined$X <- unlist(combined$X)\ncombined$Y <- unlist(combined$Y)\ncombined <- combined[order(combined$X), ]\ncombined\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           X         Y\n18 0.0000000 30.000000\n1  0.2516426 30.000000\n2  0.2516426 30.000000\n3  0.2516426 30.000000\n19 0.5000000 29.536196\n20 1.0000000 28.602454\n4  1.5161629 28.000000\n21 1.8000000 26.674847\n5  2.0864652 24.000000\n22 2.2000000 22.674847\n6  2.5935611 19.000000\n23 2.7000000 18.602454\n7  3.4436706 17.000000\n24 4.0000000 15.002045\n25 4.0000000 15.002045\n9  4.5787122 12.000000\n10 4.5787122 12.000000\n26 4.9000000 11.000000\n27 5.6000000  9.132515\n8  5.6283837  9.000000\n28 6.0000000  8.132515\n12 6.0709593  8.000000\n29 6.5000000  7.198773\n30 7.3000000  6.265031\n11 7.4986859  6.000000\n15 7.4986859  6.000000\n17 7.4986859  6.000000\n31 8.0000000  5.665644\n32 8.8000000  5.198773\n14 9.0128778  5.000000\n33 9.3000000  4.642536\n34 9.8000000  4.020041\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scatter plot of original data\nplot(data$Xi, data$Yi, pch = 16, col = \"blue\", xlab = \"Xi\", ylab = \"Yi\",\n     main = \"Monotonic Regression\")\n\nlines(combined$X, combined$Y, type = \"b\", col = \"red\", pch = 19, lwd = 2)\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Original Data\", \"Fitted Curve\"), col = c(\"blue\", \"red\"), pch = c(16, NA), lty = c(NA, 1))\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n![](static/regresi_monoton.png)\n\n# Kernel Smoothing\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Contoh 1 ---\n##Library for plots\nlibrary(ggplot2) \nlibrary(ggpubr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n##Entering data and making data in approprite form to do analysis\nx <- c(11, 22, 33, 44, 50, 56, 67, 70, 78, 89, 90, 100)\ny <- c(2337, 2750, 2301, 2500, 1700, 2100, 1100, 1750, 1000, 1642, 2000, 1932)\ndf <- data.frame(x, y)\n\n## Nadaraya-Watson bandwiths\nplot(df$x, df$y, type = \"l\")\n\nkernsmooth5 <- ksmooth(df$x, df$y, kernel=\"normal\", bandwidth=5)\nlines(kernsmooth5, lwd=2, col='blue')\n\nkernsmooth10 <- ksmooth(df$x, df$y, kernel=\"normal\", bandwidth=10)\nlines(kernsmooth10, lwd=2, col='red')\n\nlegend(\"topleft\",\n       legend = c(\"b = 5\", \"b = 10\"),\n       col = c(\"blue\", \"red\"),\n       lty = 1,\n       cex = 0.6)\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\n##Defining both gaussian and epanechinkov kernel functions\ngaussian_ker <- function(x){\n  val = (1/sqrt(2*pi))*exp(-0.5*x^2)\n  return(val)\n}\n\nepanech_ker <- function(x){\n  ind1  = ifelse((-1 <= x & x<= 1),1,0)\n  val = (3/4)*(1-x^2)*ind1\n  return(val)\n}\n\n##defining bandwidth\nh = 10\n\n##fitting\nfitted_gaussian <- c()\nfitted_epan <- c()\n\n##For loop to find fitted value for both kernel functions\nfor (i in 1:nrow(df)) {\n  x_temp <- df$x[i]\n  \n  temp_denominator_gauss <- 0\n  temp_denominator_epan <- 0\n  temp_num_gauss <- 0\n  temp_num_epan <- 0\n  \n  for (k in 1:nrow(df)) {\n    temp_denominator_gauss = temp_denominator_gauss + gaussian_ker((x_temp - df$x[k])/h)\n    temp_denominator_epan = temp_denominator_epan + epanech_ker((x_temp - df$x[k])/h)\n    temp_num_gauss = temp_num_gauss + gaussian_ker((x_temp - df$x[k])/h)*df$y[k]\n    temp_num_epan = temp_num_epan + epanech_ker((x_temp - df$x[k])/h)*df$y[k]\n  }\n  fitted_gaussian[i] = temp_num_gauss/temp_denominator_gauss\n  fitted_epan[i] = temp_num_epan/temp_denominator_epan\n}\n\nfitted_kern <- ksmooth(df$x, df$y, kernel=\"normal\", bandwidth=h, n.points = nrow(df))\nfitted_kern <- as.data.frame(fitted_kern)$y\n\ndf$fitted_gaussian <- fitted_gaussian\ndf$fitted_epan <- fitted_epan\ndf$fitted_nadaraya <- fitted_kern\n\n##Data frame for all fitted values is given as:\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     x    y fitted_gaussian fitted_epan fitted_nadaraya\n1   11 2337        2472.808    2337.000        2341.990\n2   22 2750        2515.947    2750.000        2703.484\n3   33 2301        2379.954    2301.000        2553.944\n4   44 2500        2148.260    2187.805        2315.508\n5   50 1700        2006.372    2036.842        2362.580\n6   56 2100        1836.181    1943.902        1892.487\n7   67 1100        1518.250    1409.686        1912.977\n8   70 1750        1468.214    1370.485        1392.911\n9   78 1000        1466.594    1198.529        1196.586\n10  89 1642        1682.535    1820.101        1535.150\n11  90 2000        1702.334    1821.899        1841.391\n12 100 1932        1840.908    1932.000        1930.304\n```\n\n\n:::\n\n```{.r .cell-code}\n##For plotting\np1 <- ggplot(df, aes(x = x, y = fitted_gaussian)) + geom_point() +\n  geom_point(aes(y = y), col = \"red\")+ geom_line() \n\np2 <- ggplot(df, aes(x = x, y = fitted_epan)) + geom_point() +\n  geom_point(aes(y = y), col = \"red\") + geom_line() \n\np3 <- ggplot(df, aes(x = x, y = fitted_nadaraya)) + geom_point() +\n  geom_point(aes(y = y), col = \"red\") + geom_line() \n\nggarrange(p1, p2, p3, labels  = c(\"Gaussian\", \"Epanechnikov\", \"Nadaraya-Watson\"), nrow = 1, ncol = 3)\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n:::\n\n\n# Regresi Spline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Spline regression\nDay = c(5,10,25,45,70,85,90,100,110,125,130,150,175)\nSales = c(100,125,250,300,350,460,510,460,430,400,370,340,330)\ndata = data.frame(Day,Sales)\n\nggplot(data, aes(x=Day, y = Sales)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_linear = lm(Sales ~Day, data = data)\nsummary(model_linear)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sales ~ Day, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-139.85  -93.42    3.01   54.87  164.01 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 214.8439    54.4988   3.942   0.0023 **\nDay           1.4572     0.5434   2.681   0.0214 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 100.6 on 11 degrees of freedom\nMultiple R-squared:  0.3953,\tAdjusted R-squared:  0.3403 \nF-statistic:  7.19 on 1 and 11 DF,  p-value: 0.02135\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(Sales~Day)\nlines(data$Day, predict(model_linear), col=\"red\")\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Day Sales\n1    5   100\n2   10   125\n3   25   250\n4   45   300\n5   70   350\n6   85   460\n7   90   510\n8  100   460\n9  110   430\n10 125   400\n11 130   370\n12 150   340\n13 175   330\n```\n\n\n:::\n\n```{.r .cell-code}\n#Model Spline\nmodel_spline = lm(Sales ~ bs(Day, knots = c(10, 25, 70, 90,100,150)))\nsummary(model_spline)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sales ~ bs(Day, knots = c(10, 25, 70, 90, 100, 150)))\n\nResiduals:\n         1          2          3          4          5          6          7 \n-2.240e-15 -1.402e-16  7.478e-02 -4.361e-01  2.614e+00 -1.365e+01  1.652e+01 \n         8          9         10         11         12         13 \n-7.165e+00  1.521e-01  1.027e+01 -9.040e+00  6.679e-01 -1.616e-15 \n\nCoefficients:\n                                              Estimate Std. Error t value\n(Intercept)                                     100.00      15.33   6.522\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))1    10.24      50.85   0.201\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))2    32.35      58.15   0.556\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))3   297.08      58.00   5.122\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))4    90.18      45.73   1.972\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))5   430.32      24.37  17.657\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))6   299.98      36.95   8.118\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))7   264.15      63.51   4.159\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))8   200.41      73.64   2.721\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))9   230.00      21.68  10.607\n                                              Pr(>|t|)    \n(Intercept)                                   0.007325 ** \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))1 0.853360    \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))2 0.616819    \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))3 0.014404 *  \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))4 0.143181    \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))5 0.000396 ***\nbs(Day, knots = c(10, 25, 70, 90, 100, 150))6 0.003907 ** \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))7 0.025282 *  \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))8 0.072468 .  \nbs(Day, knots = c(10, 25, 70, 90, 100, 150))9 0.001791 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.33 on 3 degrees of freedom\nMultiple R-squared:  0.9962,\tAdjusted R-squared:  0.9847 \nF-statistic: 86.64 on 9 and 3 DF,  p-value: 0.001827\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(Sales ~ Day)\nlines(data$Day, predict(model_spline), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-3.png){width=672}\n:::\n\n```{.r .cell-code}\nquantile(Day, 0.2) #20%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n20% \n 33 \n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(Day, 0.4) #40%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n40% \n 82 \n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(Day, 0.6) #60%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n60% \n102 \n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(Day, 0.8) #80%\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80% \n128 \n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_quantile = lm(Sales ~ bs(Day, knots = c(33,82,102,128)))\nsummary(model_quantile)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sales ~ bs(Day, knots = c(33, 82, 102, 128)))\n\nResiduals:\n       1        2        3        4        5        6        7        8 \n 12.0957 -22.4151  14.0619   6.0671 -38.8739  13.1709  46.8252 -18.0940 \n       9       10       11       12       13 \n-25.6499  13.3439   1.8882  -2.6963   0.2764 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              87.90      32.19   2.731 0.041227 *  \nbs(Day, knots = c(33, 82, 102, 128))1   130.44      83.55   1.561 0.179240    \nbs(Day, knots = c(33, 82, 102, 128))2   170.53      86.33   1.975 0.105201    \nbs(Day, knots = c(33, 82, 102, 128))3   316.12      72.86   4.339 0.007437 ** \nbs(Day, knots = c(33, 82, 102, 128))4   426.41      54.24   7.862 0.000535 ***\nbs(Day, knots = c(33, 82, 102, 128))5   207.36      79.64   2.604 0.048032 *  \nbs(Day, knots = c(33, 82, 102, 128))6   272.37      95.93   2.839 0.036281 *  \nbs(Day, knots = c(33, 82, 102, 128))7   241.82      47.16   5.128 0.003683 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.45 on 5 degrees of freedom\nMultiple R-squared:  0.9677,\tAdjusted R-squared:  0.9226 \nF-statistic: 21.43 on 7 and 5 DF,  p-value: 0.001915\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(model_quantile)\n```\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-6.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](modul8_files/figure-html/unnamed-chunk-27-7.png){width=672}\n:::\n:::\n",
    "supporting": [
      "modul8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}